W0418 16:35:36.826000 1916118 site-packages/torch/distributed/run.py:792] 
W0418 16:35:36.826000 1916118 site-packages/torch/distributed/run.py:792] *****************************************
W0418 16:35:36.826000 1916118 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0418 16:35:36.826000 1916118 site-packages/torch/distributed/run.py:792] *****************************************
[2025-04-18 16:35:41,137] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 16:35:41,577] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 16:35:41,840] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 16:35:41,845] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 16:35:42,428] [INFO] [comm.py:652:init_distributed] cdb=None
reward_funcs: [<function accuracy_reward at 0x7f62a3f30b80>, <function format_reward at 0x7f629d0529e0>]
[2025-04-18 16:35:42,997] [INFO] [comm.py:652:init_distributed] cdb=None

Map (num_proc=8):   0%|          | 0/2874 [00:00<?, ? examples/s]
Map (num_proc=8):  13%|█▎        | 360/2874 [00:00<00:01, 2037.32 examples/s]
Map (num_proc=8):  38%|███▊      | 1079/2874 [00:00<00:00, 3251.42 examples/s]reward_funcs: [<function accuracy_reward at 0x7f258c750b80>, <function format_reward at 0x7f255ee729e0>]

Map (num_proc=8):  63%|██████▎   | 1797/2874 [00:00<00:00, 3852.19 examples/s][2025-04-18 16:35:43,597] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-04-18 16:35:43,597] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl

Map (num_proc=8):   0%|          | 0/2874 [00:00<?, ? examples/s][2025-04-18 16:35:43,636] [INFO] [comm.py:652:init_distributed] cdb=None

Map (num_proc=8):  88%|████████▊ | 2515/2874 [00:00<00:00, 4013.07 examples/s]reward_funcs: [<function accuracy_reward at 0x7effd9b28b80>, <function format_reward at 0x7effd264e9e0>]

Map (num_proc=8):  13%|█▎        | 360/2874 [00:00<00:01, 1902.08 examples/s]
Map (num_proc=8): 100%|██████████| 2874/2874 [00:00<00:00, 3334.28 examples/s]
split ratio: 0.0
using trainer: Qwen2VLGRPOTrainer
eval strategy:no
optimizers(None, None)
[2025-04-18 16:35:43,935] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Map (num_proc=8):  38%|███▊      | 1079/2874 [00:00<00:00, 2906.88 examples/s]
Map (num_proc=8):   0%|          | 0/2874 [00:00<?, ? examples/s]reward_funcs: [<function accuracy_reward at 0x7f14b333cb80>, <function format_reward at 0x7f14ac75e9e0>]

Map (num_proc=8):  63%|██████▎   | 1797/2874 [00:00<00:00, 3364.96 examples/s]
Map (num_proc=8):  13%|█▎        | 360/2874 [00:00<00:01, 2066.75 examples/s]
Map (num_proc=8):   0%|          | 0/2874 [00:00<?, ? examples/s]
Map (num_proc=8):  88%|████████▊ | 2515/2874 [00:00<00:00, 3789.70 examples/s]
Map (num_proc=8):  38%|███▊      | 1079/2874 [00:00<00:00, 3243.06 examples/s]
Map (num_proc=8):  13%|█▎        | 360/2874 [00:00<00:01, 2024.46 examples/s]
Map (num_proc=8):  63%|██████▎   | 1797/2874 [00:00<00:00, 3771.97 examples/s]
Map (num_proc=8): 100%|██████████| 2874/2874 [00:00<00:00, 2970.84 examples/s]

Map (num_proc=8):  38%|███▊      | 1079/2874 [00:00<00:00, 3094.44 examples/s]split ratio: 0.0
using trainer: Qwen2VLGRPOTrainer
eval strategy:no
optimizers(None, None)
[2025-04-18 16:35:44,634] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Map (num_proc=8):  88%|████████▊ | 2515/2874 [00:00<00:00, 3835.04 examples/s]
Map (num_proc=8):  63%|██████▎   | 1797/2874 [00:00<00:00, 3799.12 examples/s]
Map (num_proc=8):  88%|████████▊ | 2515/2874 [00:00<00:00, 4098.71 examples/s]
Map (num_proc=8): 100%|██████████| 2874/2874 [00:00<00:00, 3183.60 examples/s]
split ratio: 0.0
using trainer: Qwen2VLGRPOTrainer
eval strategy:no
optimizers(None, None)
[2025-04-18 16:35:44,967] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`

Map (num_proc=8): 100%|██████████| 2874/2874 [00:00<00:00, 3262.42 examples/s]
split ratio: 0.0
using trainer: Qwen2VLGRPOTrainer
eval strategy:no
optimizers(None, None)
[2025-04-18 16:35:45,174] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[2025-04-18 16:35:46,341] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 729, num_elems = 8.29B

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.12s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.11s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.12s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.16s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.40s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.40s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.41s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.43s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.84s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.86s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.87s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.89s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.58s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.58s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.59s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.70s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.71s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it]

Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.71s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it]

Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.65s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  2.50s/it]
model_init_kwargs: {'attn_implementation': 'flash_attention_2', 'use_cache': False}
[2025-04-18 16:36:00,455] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
model_init_kwargs: {'attn_implementation': 'flash_attention_2', 'use_cache': False}
[2025-04-18 16:36:00,492] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
model_init_kwargs: {'attn_implementation': 'flash_attention_2', 'use_cache': False}
[2025-04-18 16:36:00,512] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
model_init_kwargs: {'attn_implementation': 'flash_attention_2', 'use_cache': False}
[2025-04-18 16:36:00,534] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-04-18 16:36:00,785] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 1458, num_elems = 16.58B

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.22s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.22s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.21s/it]
Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.19s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.41s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.42s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.44s/it]
Loading checkpoint shards:  40%|████      | 2/5 [00:07<00:10,  3.50s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.88s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.89s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.89s/it]
Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:05,  2.93s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.59s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.60s/it]
Loading checkpoint shards:  80%|████████  | 4/5 [00:11<00:02,  2.60s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.

Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.

Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  1.72s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.

Loading checkpoint shards:  80%|████████  | 4/5 [00:12<00:02,  2.67s/it]beta: 0.04
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
args: GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=local_scripts/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3/runs/Apr18_16-35-42_uubn,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_completion_length=1024,
max_grad_norm=1.0,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=6,
num_iterations=1,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_enable_prefix_caching=True,
vllm_gpu_memory_utilization=0.9,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[2025-04-18 16:36:13,448] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4

Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  1.92s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  2.53s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
beta: 0.04
beta: 0.04
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
args: GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=local_scripts/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3/runs/Apr18_16-35-43_uubn,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_completion_length=1024,
max_grad_norm=1.0,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=6,
num_iterations=1,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_enable_prefix_caching=True,
vllm_gpu_memory_utilization=0.9,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[2025-04-18 16:36:13,507] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
args: GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=local_scripts/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3/runs/Apr18_16-35-42_uubn,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_completion_length=1024,
max_grad_norm=1.0,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=6,
num_iterations=1,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_enable_prefix_caching=True,
vllm_gpu_memory_utilization=0.9,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[2025-04-18 16:36:13,509] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
beta: 0.04
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
args: GRPOConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
beta=0.04,
bf16=True,
bf16_full_eval=False,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=local_scripts/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
ds3_gather_for_generation=True,
epsilon=0.2,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_completions=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3/runs/Apr18_16-35-43_uubn,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_completion_length=1024,
max_grad_norm=1.0,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
min_p=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_generations=6,
num_iterations=1,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/data3//finetune/VLM-R1/1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
ref_model_mixup_alpha=0.6,
ref_model_sync_steps=512,
remove_unused_columns=False,
repetition_penalty=1.0,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
reward_weights=None,
run_name=1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
sync_ref_model=False,
temperature=0.9,
tf32=None,
top_k=50,
top_p=1.0,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_vllm=False,
vllm_device=auto,
vllm_dtype=auto,
vllm_enable_prefix_caching=True,
vllm_gpu_memory_utilization=0.9,
vllm_guided_decoding_regex=None,
vllm_max_model_len=None,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[2025-04-18 16:36:14,107] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed info: version=0.15.3, git-hash=unknown, git-branch=unknown
[2025-04-18 16:36:14,107] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-04-18 16:36:14,121] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-18 16:36:14,123] [INFO] [logging.py:129:log_dist] [Rank 0] Creating ZeRO Offload
[2025-04-18 16:36:14,291] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-04-18 16:36:14,292] [INFO] [utils.py:782:see_memory_usage] MA 8.38 GB         Max_MA 11.17 GB         CA 11.71 GB         Max_CA 12 GB 
[2025-04-18 16:36:14,292] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.3 GB, percent = 7.9%
Parameter Offload: Total persistent parameters: 848896 in 368 params
[2025-04-18 16:36:14,468] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-04-18 16:36:14,469] [INFO] [utils.py:782:see_memory_usage] MA 8.38 GB         Max_MA 8.38 GB         CA 11.71 GB         Max_CA 12 GB 
[2025-04-18 16:36:14,469] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 79.29 GB, percent = 7.9%
[2025-04-18 16:36:14,470] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7effb4137520>
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-04-18 16:36:14,471] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   train_batch_size ............. 8
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   world_size ................... 4
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-18 16:36:14,472] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-04-18 16:36:14,472] [INFO] [config.py:989:print_user_config]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": "auto", 
        "stage3_prefetch_bucket_size": "auto", 
        "stage3_param_persistence_threshold": "auto", 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_optimization.reduce_bucket_size": 1.284506e+07, 
    "zero_optimization.stage3_param_persistence_threshold": 3.584000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 1.156055e+07
}
Parameter Offload: Total persistent parameters: 2683904 in 424 params
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as:  (-harbin-institute-of-technology). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: / Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /data3//wandb/run-20250418_163619-fdgz8r0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1500_continue_train_after6frames_random7_bug_fix_audo_label_false_current_speed_add_ego_V_add_noType_collide_question_only_type_1_to_1_in_each_question_not_same_adjust_prompt-consine_7B_lora_64_128_0.05_lr_2e-5_deepseed_3
wandb: ⭐️ View project at https://wandb.ai/-harbin-institute-of-technology/huggingface
wandb: 🚀 View run at https://wandb.ai/-harbin-institute-of-technology/huggingface/runs/fdgz8r0k

  0%|          | 0/7180 [00:00<?, ?it/s]/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2025-04-18 16:38:29,874] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 1/7180 [02:07<254:04:10, 127.41s/it]
                                                     
{'loss': 0.0, 'grad_norm': 0.25603885378137303, 'learning_rate': 1.9999999042759967e-05, 'completion_length': 356.1458435058594, 'rewards/accuracy_reward': 0.2796737775206566, 'rewards/format_reward': 0.5625000149011612, 'reward': 0.842173844575882, 'reward_std': 0.5012924373149872, 'kl': 0.0, 'epoch': 0.0}

  0%|          | 1/7180 [02:07<254:04:10, 127.41s/it][2025-04-18 16:40:36,488] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 2/7180 [04:14<253:06:07, 126.94s/it]
                                                     
{'loss': 0.0, 'grad_norm': 0.25263590606361513, 'learning_rate': 1.9999996171040042e-05, 'completion_length': 368.1458435058594, 'rewards/accuracy_reward': 0.16887497901916504, 'rewards/format_reward': 0.7708333432674408, 'reward': 0.9397083222866058, 'reward_std': 0.34039531648159027, 'kl': 0.00067138671875, 'epoch': 0.01}

  0%|          | 2/7180 [04:14<253:06:07, 126.94s/it][2025-04-18 16:42:32,156] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 3/7180 [06:09<242:48:25, 121.79s/it]
                                                     
{'loss': 0.0001, 'grad_norm': 0.27000417440396485, 'learning_rate': 1.999999138484078e-05, 'completion_length': 318.9583435058594, 'rewards/accuracy_reward': 0.16756130754947662, 'rewards/format_reward': 0.8333333730697632, 'reward': 1.0008946061134338, 'reward_std': 0.4075225219130516, 'kl': 0.001922607421875, 'epoch': 0.01}

  0%|          | 3/7180 [06:09<242:48:25, 121.79s/it]
  0%|          | 4/7180 [08:10<242:01:24, 121.42s/it]
                                                     
{'loss': 0.0003, 'grad_norm': 0.32821256978322316, 'learning_rate': 1.9999984684163095e-05, 'completion_length': 291.7291717529297, 'rewards/accuracy_reward': 0.3435867577791214, 'rewards/format_reward': 0.9166666865348816, 'reward': 1.2602534890174866, 'reward_std': 0.4495508372783661, 'kl': 0.0064544677734375, 'epoch': 0.01}

  0%|          | 4/7180 [08:10<242:01:24, 121.42s/it][2025-04-18 16:46:32,574] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 5/7180 [10:10<240:40:17, 120.76s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.3668544597793481, 'learning_rate': 1.999997606900827e-05, 'completion_length': 289.75001525878906, 'rewards/accuracy_reward': 0.18613497912883759, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.1653016805648804, 'reward_std': 0.29668595641851425, 'kl': 0.0120849609375, 'epoch': 0.01}

  0%|          | 5/7180 [10:10<240:40:17, 120.76s/it]
  0%|          | 6/7180 [12:19<246:43:28, 123.81s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.32930052585222497, 'learning_rate': 1.9999965539377956e-05, 'completion_length': 276.18751525878906, 'rewards/accuracy_reward': 0.3280537948012352, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3072205185890198, 'reward_std': 0.4462152123451233, 'kl': 0.02093505859375, 'epoch': 0.02}

  0%|          | 6/7180 [12:19<246:43:28, 123.81s/it]
  0%|          | 7/7180 [14:11<238:55:26, 119.91s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.38547475179383195, 'learning_rate': 1.999995309527417e-05, 'completion_length': 222.56250762939453, 'rewards/accuracy_reward': 0.22465740889310837, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.2038241028785706, 'reward_std': 0.35244083404541016, 'kl': 0.0299072265625, 'epoch': 0.02}

  0%|          | 7/7180 [14:11<238:55:26, 119.91s/it]
  0%|          | 8/7180 [15:50<225:11:22, 113.03s/it]
                                                     
{'loss': 0.0018, 'grad_norm': 0.3931858747473919, 'learning_rate': 1.9999938736699288e-05, 'completion_length': 188.6666717529297, 'rewards/accuracy_reward': 0.4037519693374634, 'rewards/format_reward': 1.0, 'reward': 1.4037519693374634, 'reward_std': 0.3606206178665161, 'kl': 0.0445556640625, 'epoch': 0.02}

  0%|          | 8/7180 [15:50<225:11:22, 113.03s/it]
  0%|          | 9/7180 [17:30<217:33:28, 109.22s/it]
                                                     
{'loss': 0.0022, 'grad_norm': 0.38784910591673927, 'learning_rate': 1.999992246365607e-05, 'completion_length': 203.64583587646484, 'rewards/accuracy_reward': 0.24768152832984924, 'rewards/format_reward': 1.0, 'reward': 1.2476814985275269, 'reward_std': 0.3355880230665207, 'kl': 0.055419921875, 'epoch': 0.03}

  0%|          | 9/7180 [17:30<217:33:28, 109.22s/it]
  0%|          | 10/7180 [19:25<221:00:34, 110.97s/it]
                                                      
{'loss': 0.0022, 'grad_norm': 0.3385799025988644, 'learning_rate': 1.999990427614762e-05, 'completion_length': 200.2916717529297, 'rewards/accuracy_reward': 0.3407844640314579, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3199511766433716, 'reward_std': 0.31964799761772156, 'kl': 0.0537109375, 'epoch': 0.03}

  0%|          | 10/7180 [19:25<221:00:34, 110.97s/it]
  0%|          | 11/7180 [21:12<218:13:06, 109.58s/it]
                                                      
{'loss': 0.0029, 'grad_norm': 0.4033837553075562, 'learning_rate': 1.9999884174177426e-05, 'completion_length': 175.6666717529297, 'rewards/accuracy_reward': 0.30946865677833557, 'rewards/format_reward': 1.0, 'reward': 1.309468686580658, 'reward_std': 0.2984970510005951, 'kl': 0.072021484375, 'epoch': 0.03}

  0%|          | 11/7180 [21:12<218:13:06, 109.58s/it]
  0%|          | 12/7180 [22:48<210:19:41, 105.63s/it]
                                                      
{'loss': 0.0023, 'grad_norm': 0.35666294104802626, 'learning_rate': 1.9999862157749333e-05, 'completion_length': 201.45833587646484, 'rewards/accuracy_reward': 0.19570587575435638, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.1748725771903992, 'reward_std': 0.3161795064806938, 'kl': 0.0582275390625, 'epoch': 0.03}

  0%|          | 12/7180 [22:48<210:19:41, 105.63s/it]
  0%|          | 13/7180 [24:33<209:52:22, 105.42s/it]
                                                      
{'loss': 0.0016, 'grad_norm': 0.3443535633828956, 'learning_rate': 1.999983822686756e-05, 'completion_length': 238.39583587646484, 'rewards/accuracy_reward': 0.37302038073539734, 'rewards/format_reward': 1.0, 'reward': 1.3730204701423645, 'reward_std': 0.3047052472829819, 'kl': 0.039794921875, 'epoch': 0.04}

  0%|          | 13/7180 [24:33<209:52:22, 105.42s/it]
  0%|          | 14/7180 [26:11<205:13:19, 103.10s/it]
                                                      
{'loss': 0.0017, 'grad_norm': 0.35885023415784856, 'learning_rate': 1.9999812381536685e-05, 'completion_length': 226.6666717529297, 'rewards/accuracy_reward': 0.25885170698165894, 'rewards/format_reward': 1.0, 'reward': 1.2588517665863037, 'reward_std': 0.35015322268009186, 'kl': 0.0423583984375, 'epoch': 0.04}

  0%|          | 14/7180 [26:11<205:13:19, 103.10s/it]
  0%|          | 15/7180 [27:53<204:23:03, 102.69s/it]
                                                      
{'loss': 0.0015, 'grad_norm': 0.3023537659078586, 'learning_rate': 1.999978462176166e-05, 'completion_length': 257.68750762939453, 'rewards/accuracy_reward': 0.22713685780763626, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.2063035368919373, 'reward_std': 0.26673756539821625, 'kl': 0.0379638671875, 'epoch': 0.04}

  0%|          | 15/7180 [27:53<204:23:03, 102.69s/it]
  0%|          | 16/7180 [29:42<208:09:21, 104.60s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.3117941157719319, 'learning_rate': 1.9999754947547794e-05, 'completion_length': 282.87501525878906, 'rewards/accuracy_reward': 0.2991076409816742, 'rewards/format_reward': 1.0, 'reward': 1.2991076707839966, 'reward_std': 0.3611433804035187, 'kl': 0.0333251953125, 'epoch': 0.04}

  0%|          | 16/7180 [29:42<208:09:21, 104.60s/it]
  0%|          | 17/7180 [31:35<213:10:30, 107.14s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.29085292619530206, 'learning_rate': 1.9999723358900776e-05, 'completion_length': 311.06251525878906, 'rewards/accuracy_reward': 0.19556062668561935, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.1747273206710815, 'reward_std': 0.346929132938385, 'kl': 0.0264892578125, 'epoch': 0.05}

  0%|          | 17/7180 [31:35<213:10:30, 107.14s/it]
  0%|          | 18/7180 [33:18<210:38:31, 105.88s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.33230312869517914, 'learning_rate': 1.9999689855826644e-05, 'completion_length': 293.18751525878906, 'rewards/accuracy_reward': 0.32937484979629517, 'rewards/format_reward': 1.0, 'reward': 1.32937490940094, 'reward_std': 0.31801843643188477, 'kl': 0.02618408203125, 'epoch': 0.05}

  0%|          | 18/7180 [33:18<210:38:31, 105.88s/it]
  0%|          | 19/7180 [35:15<217:28:42, 109.33s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.3317205879535444, 'learning_rate': 1.999965443833182e-05, 'completion_length': 307.0416717529297, 'rewards/accuracy_reward': 0.3229324221611023, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3020991086959839, 'reward_std': 0.3968846797943115, 'kl': 0.02392578125, 'epoch': 0.05}

  0%|          | 19/7180 [35:15<217:28:42, 109.33s/it]
  0%|          | 20/7180 [37:18<225:29:16, 113.37s/it]
                                                      
{'loss': 0.0008, 'grad_norm': 0.270601774305062, 'learning_rate': 1.999961710642308e-05, 'completion_length': 374.6666717529297, 'rewards/accuracy_reward': 0.20770824700593948, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.1660416722297668, 'reward_std': 0.32079239189624786, 'kl': 0.020751953125, 'epoch': 0.06}

  0%|          | 20/7180 [37:18<225:29:16, 113.37s/it]
  0%|          | 21/7180 [39:44<245:11:11, 123.30s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.2893194848852855, 'learning_rate': 1.9999577860107576e-05, 'completion_length': 343.8541717529297, 'rewards/accuracy_reward': 0.45186659693717957, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.4101999998092651, 'reward_std': 0.43287982791662216, 'kl': 0.0213623046875, 'epoch': 0.06}

  0%|          | 21/7180 [39:44<245:11:11, 123.30s/it]
  0%|          | 22/7180 [42:58<287:26:50, 144.57s/it]
                                                      
{'loss': 0.0008, 'grad_norm': 0.2432416827982285, 'learning_rate': 1.9999536699392817e-05, 'completion_length': 356.6458435058594, 'rewards/accuracy_reward': 0.22736957669258118, 'rewards/format_reward': 1.0, 'reward': 1.2273696064949036, 'reward_std': 0.23031610995531082, 'kl': 0.02008056640625, 'epoch': 0.06}

  0%|          | 22/7180 [42:59<287:26:50, 144.57s/it]
  0%|          | 23/7180 [46:12<316:53:36, 159.40s/it]
                                                      
{'loss': 0.0008, 'grad_norm': 0.2606568083490096, 'learning_rate': 1.9999493624286682e-05, 'completion_length': 336.56251525878906, 'rewards/accuracy_reward': 0.2813384309411049, 'rewards/format_reward': 1.0, 'reward': 1.2813384532928467, 'reward_std': 0.31969644129276276, 'kl': 0.02020263671875, 'epoch': 0.06}

  0%|          | 23/7180 [46:13<316:53:36, 159.40s/it][2025-04-18 17:25:54,683] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 24/7180 [49:32<340:36:20, 171.35s/it]
                                                      
{'loss': 0.0007, 'grad_norm': 0.26208508178420553, 'learning_rate': 1.9999448634797423e-05, 'completion_length': 372.5833435058594, 'rewards/accuracy_reward': 0.3268418461084366, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.306008517742157, 'reward_std': 0.37967218458652496, 'kl': 0.01763916015625, 'epoch': 0.07}

  0%|          | 24/7180 [49:32<340:36:20, 171.35s/it]
  0%|          | 25/7180 [52:34<347:00:36, 174.60s/it]
                                                      
{'loss': 0.0007, 'grad_norm': 0.2706480693904201, 'learning_rate': 1.999940173093365e-05, 'completion_length': 351.6458435058594, 'rewards/accuracy_reward': 0.27566541731357574, 'rewards/format_reward': 1.0, 'reward': 1.2756654620170593, 'reward_std': 0.3745543211698532, 'kl': 0.01849365234375, 'epoch': 0.07}

  0%|          | 25/7180 [52:34<347:00:36, 174.60s/it][2025-04-18 17:32:32,699] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 26/7180 [56:10<371:33:19, 186.97s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.25400725140923114, 'learning_rate': 1.9999352912704342e-05, 'completion_length': 384.2916717529297, 'rewards/accuracy_reward': 0.4188220053911209, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3979887962341309, 'reward_std': 0.36822155117988586, 'kl': 0.02362060546875, 'epoch': 0.07}

  0%|          | 26/7180 [56:10<371:33:19, 186.97s/it][2025-04-18 17:36:03,023] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 27/7180 [59:40<385:25:31, 193.98s/it]
                                                      
{'loss': 0.0007, 'grad_norm': 0.2548960767584509, 'learning_rate': 1.9999302180118848e-05, 'completion_length': 376.2708435058594, 'rewards/accuracy_reward': 0.34830813854932785, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.32747483253479, 'reward_std': 0.36702860891819, 'kl': 0.01806640625, 'epoch': 0.08}

  0%|          | 27/7180 [59:40<385:25:31, 193.98s/it]
  0%|          | 28/7180 [1:02:57<386:59:17, 194.79s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.22144142770570815, 'learning_rate': 1.9999249533186875e-05, 'completion_length': 389.9583435058594, 'rewards/accuracy_reward': 0.25315993279218674, 'rewards/format_reward': 1.0, 'reward': 1.2531599402427673, 'reward_std': 0.22524430602788925, 'kl': 0.0162353515625, 'epoch': 0.08}

  0%|          | 28/7180 [1:02:57<386:59:17, 194.79s/it]
  0%|          | 29/7180 [1:06:10<385:45:43, 194.20s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2685018105544898, 'learning_rate': 1.9999194971918506e-05, 'completion_length': 352.9583435058594, 'rewards/accuracy_reward': 0.5330333560705185, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.512200117111206, 'reward_std': 0.4121178090572357, 'kl': 0.0174560546875, 'epoch': 0.08}

  0%|          | 29/7180 [1:06:10<385:45:43, 194.20s/it]
  0%|          | 30/7180 [1:09:56<404:56:39, 203.89s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.23497572648026654, 'learning_rate': 1.999913849632419e-05, 'completion_length': 406.0208435058594, 'rewards/accuracy_reward': 0.2938487231731415, 'rewards/format_reward': 1.0, 'reward': 1.2938487529754639, 'reward_std': 0.334257110953331, 'kl': 0.015167236328125, 'epoch': 0.08}

  0%|          | 30/7180 [1:09:56<404:56:39, 203.89s/it]
  0%|          | 31/7180 [1:13:21<405:20:13, 204.11s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.27553094869138445, 'learning_rate': 1.9999080106414736e-05, 'completion_length': 399.1666717529297, 'rewards/accuracy_reward': 0.3779238760471344, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.33625727891922, 'reward_std': 0.4204464852809906, 'kl': 0.0157470703125, 'epoch': 0.09}

  0%|          | 31/7180 [1:13:21<405:20:13, 204.11s/it]
  0%|          | 32/7180 [1:16:47<406:30:46, 204.74s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2283902273328685, 'learning_rate': 1.9999019802201318e-05, 'completion_length': 391.1458435058594, 'rewards/accuracy_reward': 0.4526231288909912, 'rewards/format_reward': 1.0, 'reward': 1.452623188495636, 'reward_std': 0.33811089396476746, 'kl': 0.0169677734375, 'epoch': 0.09}

  0%|          | 32/7180 [1:16:47<406:30:46, 204.74s/it]
  0%|          | 33/7180 [1:20:12<406:22:59, 204.70s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.24192319309878857, 'learning_rate': 1.999895758369549e-05, 'completion_length': 405.3958435058594, 'rewards/accuracy_reward': 0.43142130970954895, 'rewards/format_reward': 1.0, 'reward': 1.4314212799072266, 'reward_std': 0.4061700999736786, 'kl': 0.0162353515625, 'epoch': 0.09}

  0%|          | 33/7180 [1:20:12<406:22:59, 204.70s/it]
  0%|          | 34/7180 [1:23:25<399:51:23, 201.44s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.28624403271861554, 'learning_rate': 1.9998893450909157e-05, 'completion_length': 379.81251525878906, 'rewards/accuracy_reward': 0.30897916853427887, 'rewards/format_reward': 1.0, 'reward': 1.3089792132377625, 'reward_std': 0.4230395257472992, 'kl': 0.01934814453125, 'epoch': 0.09}

  0%|          | 34/7180 [1:23:25<399:51:23, 201.44s/it]
  0%|          | 35/7180 [1:26:57<405:44:32, 204.43s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2588991430463634, 'learning_rate': 1.9998827403854596e-05, 'completion_length': 418.85418701171875, 'rewards/accuracy_reward': 0.33322376012802124, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3123905062675476, 'reward_std': 0.38889387249946594, 'kl': 0.01739501953125, 'epoch': 0.1}

  0%|          | 35/7180 [1:26:57<405:44:32, 204.43s/it]
  1%|          | 36/7180 [1:30:20<405:10:50, 204.18s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.21979204827628174, 'learning_rate': 1.9998759442544463e-05, 'completion_length': 386.16668701171875, 'rewards/accuracy_reward': 0.42984001338481903, 'rewards/format_reward': 1.0, 'reward': 1.4298400282859802, 'reward_std': 0.21823570132255554, 'kl': 0.0206298828125, 'epoch': 0.1}

  1%|          | 36/7180 [1:30:20<405:10:50, 204.18s/it]
  1%|          | 37/7180 [1:33:01<379:30:03, 191.26s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.2803969281731368, 'learning_rate': 1.9998689566991755e-05, 'completion_length': 397.7916717529297, 'rewards/accuracy_reward': 0.5083962678909302, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4875630140304565, 'reward_std': 0.4197576493024826, 'kl': 0.0218505859375, 'epoch': 0.1}

  1%|          | 37/7180 [1:33:01<379:30:03, 191.26s/it]
  1%|          | 38/7180 [1:35:14<344:37:11, 173.71s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2583836949501005, 'learning_rate': 1.9998617777209855e-05, 'completion_length': 422.41668701171875, 'rewards/accuracy_reward': 0.2700163647532463, 'rewards/format_reward': 1.0, 'reward': 1.2700164318084717, 'reward_std': 0.41002632677555084, 'kl': 0.01995849609375, 'epoch': 0.11}

  1%|          | 38/7180 [1:35:14<344:37:11, 173.71s/it]
  1%|          | 39/7180 [1:38:45<366:49:44, 184.93s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.24113915700477315, 'learning_rate': 1.9998544073212505e-05, 'completion_length': 428.0208435058594, 'rewards/accuracy_reward': 0.31127452850341797, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.2696079015731812, 'reward_std': 0.41561006009578705, 'kl': 0.01983642578125, 'epoch': 0.11}

  1%|          | 39/7180 [1:38:45<366:49:44, 184.93s/it]
  1%|          | 40/7180 [1:42:22<385:27:23, 194.35s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.23141691848357582, 'learning_rate': 1.9998468455013825e-05, 'completion_length': 441.12501525878906, 'rewards/accuracy_reward': 0.3920401930809021, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.3503735661506653, 'reward_std': 0.3315342962741852, 'kl': 0.021728515625, 'epoch': 0.11}

  1%|          | 40/7180 [1:42:22<385:27:23, 194.35s/it]
  1%|          | 41/7180 [1:45:55<396:52:11, 200.13s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.22709300902462198, 'learning_rate': 1.999839092262828e-05, 'completion_length': 433.9166717529297, 'rewards/accuracy_reward': 0.2913763150572777, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.2705430388450623, 'reward_std': 0.3781401962041855, 'kl': 0.01983642578125, 'epoch': 0.11}

  1%|          | 41/7180 [1:45:55<396:52:11, 200.13s/it]
  1%|          | 42/7180 [1:49:40<411:30:58, 207.55s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2609714232868847, 'learning_rate': 1.999831147607072e-05, 'completion_length': 444.5208435058594, 'rewards/accuracy_reward': 0.41633960604667664, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.3746730089187622, 'reward_std': 0.4976237118244171, 'kl': 0.020751953125, 'epoch': 0.12}

  1%|          | 42/7180 [1:49:40<411:30:58, 207.55s/it]
  1%|          | 43/7180 [1:53:14<414:57:38, 209.31s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.23904227309385573, 'learning_rate': 1.999823011535635e-05, 'completion_length': 444.6458435058594, 'rewards/accuracy_reward': 0.37336602807044983, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.3316994309425354, 'reward_std': 0.5234161019325256, 'kl': 0.01922607421875, 'epoch': 0.12}

  1%|          | 43/7180 [1:53:14<414:57:38, 209.31s/it]
  1%|          | 44/7180 [1:56:48<418:08:30, 210.95s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.238190168277393, 'learning_rate': 1.999814684050075e-05, 'completion_length': 442.8125, 'rewards/accuracy_reward': 0.4211377054452896, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4003044366836548, 'reward_std': 0.41781291365623474, 'kl': 0.01983642578125, 'epoch': 0.12}

  1%|          | 44/7180 [1:56:48<418:08:30, 210.95s/it]
  1%|          | 45/7180 [2:00:24<420:54:25, 212.37s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.22625435443654363, 'learning_rate': 1.9998061651519868e-05, 'completion_length': 423.1041717529297, 'rewards/accuracy_reward': 0.4317174404859543, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.3900508880615234, 'reward_std': 0.3617425188422203, 'kl': 0.021240234375, 'epoch': 0.13}

  1%|          | 45/7180 [2:00:24<420:54:25, 212.37s/it]
  1%|          | 46/7180 [2:03:57<421:22:37, 212.64s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.2325628569606353, 'learning_rate': 1.9997974548430004e-05, 'completion_length': 435.5833435058594, 'rewards/accuracy_reward': 0.4671407639980316, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4463074803352356, 'reward_std': 0.385202020406723, 'kl': 0.021240234375, 'epoch': 0.13}

  1%|          | 46/7180 [2:03:57<421:22:37, 212.64s/it]
  1%|          | 47/7180 [2:07:18<414:29:21, 209.19s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.2653651597099629, 'learning_rate': 1.9997885531247836e-05, 'completion_length': 400.3958435058594, 'rewards/accuracy_reward': 0.2646770626306534, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.2438437342643738, 'reward_std': 0.42059285938739777, 'kl': 0.02337646484375, 'epoch': 0.13}

  1%|          | 47/7180 [2:07:18<414:29:21, 209.19s/it][2025-04-18 18:47:34,264] [WARNING] [stage3.py:2105:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 48/7180 [2:11:11<428:30:32, 216.30s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.23063013864798865, 'learning_rate': 1.9997794599990405e-05, 'completion_length': 422.4791717529297, 'rewards/accuracy_reward': 0.5145450830459595, 'rewards/format_reward': 1.0, 'reward': 1.5145450830459595, 'reward_std': 0.3745621293783188, 'kl': 0.01776123046875, 'epoch': 0.13}

  1%|          | 48/7180 [2:11:11<428:30:32, 216.30s/it]
  1%|          | 49/7180 [2:14:46<427:24:29, 215.77s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.22965298725105968, 'learning_rate': 1.9997701754675124e-05, 'completion_length': 424.54168701171875, 'rewards/accuracy_reward': 0.37602828443050385, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3551949858665466, 'reward_std': 0.37006690353155136, 'kl': 0.02178955078125, 'epoch': 0.14}

  1%|          | 49/7180 [2:14:46<427:24:29, 215.77s/it]
  1%|          | 50/7180 [2:18:00<414:40:15, 209.37s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.25478150861282073, 'learning_rate': 1.999760699531977e-05, 'completion_length': 399.5208435058594, 'rewards/accuracy_reward': 0.3539225161075592, 'rewards/format_reward': 1.0, 'reward': 1.3539225459098816, 'reward_std': 0.46274007856845856, 'kl': 0.0208740234375, 'epoch': 0.14}

  1%|          | 50/7180 [2:18:00<414:40:15, 209.37s/it]
  1%|          | 51/7180 [2:21:43<422:44:14, 213.47s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.22251119092735502, 'learning_rate': 1.999751032194248e-05, 'completion_length': 446.1666717529297, 'rewards/accuracy_reward': 0.41421568393707275, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3933824300765991, 'reward_std': 0.3841731995344162, 'kl': 0.017578125, 'epoch': 0.14}

  1%|          | 51/7180 [2:21:43<422:44:14, 213.47s/it]
  1%|          | 52/7180 [2:25:27<428:56:24, 216.64s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.20788145668503133, 'learning_rate': 1.9997411734561757e-05, 'completion_length': 418.54168701171875, 'rewards/accuracy_reward': 0.4456699788570404, 'rewards/format_reward': 0.9375000298023224, 'reward': 1.3831700086593628, 'reward_std': 0.38846707344055176, 'kl': 0.0205078125, 'epoch': 0.14}

  1%|          | 52/7180 [2:25:27<428:56:24, 216.64s/it]
  1%|          | 53/7180 [2:29:10<432:28:18, 218.45s/it]
                                                        
{'loss': 0.0011, 'grad_norm': 0.24263980668650995, 'learning_rate': 1.9997311233196484e-05, 'completion_length': 409.5208435058594, 'rewards/accuracy_reward': 0.3645687997341156, 'rewards/format_reward': 1.0, 'reward': 1.364568829536438, 'reward_std': 0.3689030110836029, 'kl': 0.02630615234375, 'epoch': 0.15}

  1%|          | 53/7180 [2:29:10<432:28:18, 218.45s/it]
  1%|          | 54/7180 [2:32:30<421:19:34, 212.85s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.265338839928442, 'learning_rate': 1.9997208817865897e-05, 'completion_length': 413.6041717529297, 'rewards/accuracy_reward': 0.45710787177085876, 'rewards/format_reward': 1.0, 'reward': 1.4571079015731812, 'reward_std': 0.5131087154150009, 'kl': 0.02166748046875, 'epoch': 0.15}

  1%|          | 54/7180 [2:32:30<421:19:34, 212.85s/it]
  1%|          | 55/7180 [2:36:01<420:31:45, 212.48s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2351298858500204, 'learning_rate': 1.9997104488589607e-05, 'completion_length': 398.5, 'rewards/accuracy_reward': 0.4138071984052658, 'rewards/format_reward': 1.0, 'reward': 1.4138072729110718, 'reward_std': 0.2693534716963768, 'kl': 0.0211181640625, 'epoch': 0.15}

  1%|          | 55/7180 [2:36:01<420:31:45, 212.48s/it]
  1%|          | 56/7180 [2:39:24<414:46:49, 209.60s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.24979580336647947, 'learning_rate': 1.9996998245387586e-05, 'completion_length': 429.0208435058594, 'rewards/accuracy_reward': 0.37377452850341797, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3529412150382996, 'reward_std': 0.4785580039024353, 'kl': 0.019775390625, 'epoch': 0.16}

  1%|          | 56/7180 [2:39:24<414:46:49, 209.60s/it]
  1%|          | 57/7180 [2:42:55<415:08:14, 209.81s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2629779801066631, 'learning_rate': 1.999689008828017e-05, 'completion_length': 404.5833435058594, 'rewards/accuracy_reward': 0.5388341844081879, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.4971676468849182, 'reward_std': 0.34537968039512634, 'kl': 0.02099609375, 'epoch': 0.16}

  1%|          | 57/7180 [2:42:55<415:08:14, 209.81s/it]
  1%|          | 58/7180 [2:46:17<410:48:30, 207.65s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.25002187120154906, 'learning_rate': 1.9996780017288068e-05, 'completion_length': 402.9791717529297, 'rewards/accuracy_reward': 0.671271562576294, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.6504382491111755, 'reward_std': 0.3854690492153168, 'kl': 0.0198974609375, 'epoch': 0.16}

  1%|          | 58/7180 [2:46:17<410:48:30, 207.65s/it]
  1%|          | 59/7180 [2:49:34<404:19:33, 204.41s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.24955700937045786, 'learning_rate': 1.999666803243236e-05, 'completion_length': 388.62501525878906, 'rewards/accuracy_reward': 0.49243342876434326, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.4507667422294617, 'reward_std': 0.40987735986709595, 'kl': 0.0211181640625, 'epoch': 0.16}

  1%|          | 59/7180 [2:49:34<404:19:33, 204.41s/it]
  1%|          | 60/7180 [2:53:11<411:46:09, 208.20s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2618291250237568, 'learning_rate': 1.9996554133734473e-05, 'completion_length': 394.5208435058594, 'rewards/accuracy_reward': 0.3485300540924072, 'rewards/format_reward': 1.0, 'reward': 1.348530113697052, 'reward_std': 0.3862355202436447, 'kl': 0.0208740234375, 'epoch': 0.17}

  1%|          | 60/7180 [2:53:11<411:46:09, 208.20s/it]
  1%|          | 61/7180 [2:56:32<407:39:55, 206.15s/it]
                                                        
{'loss': 0.0009, 'grad_norm': 0.2147771065388076, 'learning_rate': 1.9996438321216223e-05, 'completion_length': 380.2708435058594, 'rewards/accuracy_reward': 0.3697943836450577, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3489611148834229, 'reward_std': 0.2525175213813782, 'kl': 0.02178955078125, 'epoch': 0.17}

  1%|          | 61/7180 [2:56:32<407:39:55, 206.15s/it]
  1%|          | 62/7180 [2:59:48<401:18:59, 202.97s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.265717484733023, 'learning_rate': 1.9996320594899775e-05, 'completion_length': 368.2708435058594, 'rewards/accuracy_reward': 0.44186604022979736, 'rewards/format_reward': 0.9375000298023224, 'reward': 1.3793660998344421, 'reward_std': 0.34304553270339966, 'kl': 0.0203857421875, 'epoch': 0.17}

  1%|          | 62/7180 [2:59:48<401:18:59, 202.97s/it]
  1%|          | 63/7180 [3:03:09<399:59:15, 202.33s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2090990651121923, 'learning_rate': 1.999620095480767e-05, 'completion_length': 386.18751525878906, 'rewards/accuracy_reward': 0.32921361923217773, 'rewards/format_reward': 1.0, 'reward': 1.3292136192321777, 'reward_std': 0.27665508911013603, 'kl': 0.01788330078125, 'epoch': 0.18}

  1%|          | 63/7180 [3:03:09<399:59:15, 202.33s/it]
  1%|          | 64/7180 [3:06:53<412:41:41, 208.78s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.24982605215939213, 'learning_rate': 1.9996079400962818e-05, 'completion_length': 373.0625, 'rewards/accuracy_reward': 0.2806304395198822, 'rewards/format_reward': 1.0, 'reward': 1.2806305289268494, 'reward_std': 0.43382759392261505, 'kl': 0.01849365234375, 'epoch': 0.18}

  1%|          | 64/7180 [3:06:53<412:41:41, 208.78s/it]
  1%|          | 65/7180 [3:10:13<407:49:46, 206.35s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.23949800386956765, 'learning_rate': 1.999595593338848e-05, 'completion_length': 369.56251525878906, 'rewards/accuracy_reward': 0.2851620763540268, 'rewards/format_reward': 1.0, 'reward': 1.2851621508598328, 'reward_std': 0.3817870020866394, 'kl': 0.01739501953125, 'epoch': 0.18}

  1%|          | 65/7180 [3:10:13<407:49:46, 206.35s/it]
  1%|          | 66/7180 [3:13:44<410:33:27, 207.76s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.24530753743493386, 'learning_rate': 1.99958305521083e-05, 'completion_length': 383.2083435058594, 'rewards/accuracy_reward': 0.4797864705324173, 'rewards/format_reward': 1.0, 'reward': 1.4797865152359009, 'reward_std': 0.36323632299900055, 'kl': 0.01861572265625, 'epoch': 0.18}

  1%|          | 66/7180 [3:13:44<410:33:27, 207.76s/it]
  1%|          | 67/7180 [3:16:46<394:56:39, 199.89s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.24860974075077813, 'learning_rate': 1.9995703257146288e-05, 'completion_length': 352.5, 'rewards/accuracy_reward': 0.23145103454589844, 'rewards/format_reward': 1.0, 'reward': 1.2314510941505432, 'reward_std': 0.3523377925157547, 'kl': 0.01654052734375, 'epoch': 0.19}

  1%|          | 67/7180 [3:16:46<394:56:39, 199.89s/it]
  1%|          | 68/7180 [3:20:16<400:44:46, 202.85s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2467458574740874, 'learning_rate': 1.9995574048526796e-05, 'completion_length': 358.3541717529297, 'rewards/accuracy_reward': 0.47639361023902893, 'rewards/format_reward': 1.0, 'reward': 1.4763935804367065, 'reward_std': 0.39196112751960754, 'kl': 0.016845703125, 'epoch': 0.19}

  1%|          | 68/7180 [3:20:16<400:44:46, 202.85s/it]
  1%|          | 69/7180 [3:23:23<391:33:46, 198.23s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.19859446456496088, 'learning_rate': 1.9995442926274584e-05, 'completion_length': 358.7083435058594, 'rewards/accuracy_reward': 0.37459151446819305, 'rewards/format_reward': 1.0, 'reward': 1.3745915293693542, 'reward_std': 0.3206673115491867, 'kl': 0.0150146484375, 'epoch': 0.19}

  1%|          | 69/7180 [3:23:23<391:33:46, 198.23s/it]
  1%|          | 70/7180 [3:26:49<395:47:20, 200.40s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2694358062750001, 'learning_rate': 1.9995309890414735e-05, 'completion_length': 348.1458435058594, 'rewards/accuracy_reward': 0.34007419645786285, 'rewards/format_reward': 1.0, 'reward': 1.3400742411613464, 'reward_std': 0.441219225525856, 'kl': 0.016632080078125, 'epoch': 0.19}

  1%|          | 70/7180 [3:26:49<395:47:20, 200.40s/it]
  1%|          | 71/7180 [3:30:18<400:59:34, 203.06s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.2580956399229161, 'learning_rate': 1.9995174940972726e-05, 'completion_length': 344.2708435058594, 'rewards/accuracy_reward': 0.3208589255809784, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.2791922092437744, 'reward_std': 0.4691503047943115, 'kl': 0.015380859375, 'epoch': 0.2}

  1%|          | 71/7180 [3:30:18<400:59:34, 203.06s/it]
  1%|          | 72/7180 [3:33:38<399:07:11, 202.14s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.26202511512852694, 'learning_rate': 1.99950380779744e-05, 'completion_length': 349.5, 'rewards/accuracy_reward': 0.44171570241451263, 'rewards/format_reward': 1.0, 'reward': 1.4417157769203186, 'reward_std': 0.3617486506700516, 'kl': 0.017486572265625, 'epoch': 0.2}

  1%|          | 72/7180 [3:33:38<399:07:11, 202.14s/it]
  1%|          | 73/7180 [3:36:46<390:57:28, 198.04s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.23413053863498237, 'learning_rate': 1.9994899301445946e-05, 'completion_length': 369.2083435058594, 'rewards/accuracy_reward': 0.2788635306060314, 'rewards/format_reward': 1.0, 'reward': 1.278863549232483, 'reward_std': 0.33967770636081696, 'kl': 0.014495849609375, 'epoch': 0.2}

  1%|          | 73/7180 [3:36:46<390:57:28, 198.04s/it]
  1%|          | 74/7180 [3:40:01<389:09:23, 197.15s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.23184738086577017, 'learning_rate': 1.9994758611413944e-05, 'completion_length': 355.3958435058594, 'rewards/accuracy_reward': 0.2901773154735565, 'rewards/format_reward': 1.0, 'reward': 1.2901774048805237, 'reward_std': 0.35855086147785187, 'kl': 0.0166015625, 'epoch': 0.21}

  1%|          | 74/7180 [3:40:01<389:09:23, 197.15s/it]
  1%|          | 75/7180 [3:43:26<393:46:24, 199.52s/it]
                                                        
{'loss': 0.0005, 'grad_norm': 0.2318958223607603, 'learning_rate': 1.9994616007905318e-05, 'completion_length': 380.6041717529297, 'rewards/accuracy_reward': 0.26400026679039, 'rewards/format_reward': 1.0, 'reward': 1.2640002965927124, 'reward_std': 0.3597968965768814, 'kl': 0.013336181640625, 'epoch': 0.21}

  1%|          | 75/7180 [3:43:26<393:46:24, 199.52s/it]
  1%|          | 76/7180 [3:46:39<389:33:01, 197.41s/it]
                                                        
{'loss': 0.0005, 'grad_norm': 0.22167642130374882, 'learning_rate': 1.999447149094738e-05, 'completion_length': 371.75001525878906, 'rewards/accuracy_reward': 0.40476468205451965, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3839313387870789, 'reward_std': 0.44928812980651855, 'kl': 0.0135498046875, 'epoch': 0.21}

  1%|          | 76/7180 [3:46:39<389:33:01, 197.41s/it]
  1%|          | 77/7180 [3:49:56<389:20:46, 197.33s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.2182493076863193, 'learning_rate': 1.9994325060567793e-05, 'completion_length': 355.0416717529297, 'rewards/accuracy_reward': 0.3941993713378906, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.373366117477417, 'reward_std': 0.31362029910087585, 'kl': 0.01385498046875, 'epoch': 0.21}

  1%|          | 77/7180 [3:49:56<389:20:46, 197.33s/it]
  1%|          | 78/7180 [3:52:53<377:15:41, 191.23s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2530329739971831, 'learning_rate': 1.9994176716794588e-05, 'completion_length': 341.8958435058594, 'rewards/accuracy_reward': 0.4765717387199402, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4557385444641113, 'reward_std': 0.36932967603206635, 'kl': 0.01629638671875, 'epoch': 0.22}

  1%|          | 78/7180 [3:52:53<377:15:41, 191.23s/it]
  1%|          | 79/7180 [3:54:46<330:43:38, 167.67s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.24399377961004792, 'learning_rate': 1.9994026459656167e-05, 'completion_length': 344.2291717529297, 'rewards/accuracy_reward': 0.514890193939209, 'rewards/format_reward': 1.0, 'reward': 1.514890193939209, 'reward_std': 0.45146310329437256, 'kl': 0.01470947265625, 'epoch': 0.22}

  1%|          | 79/7180 [3:54:46<330:43:38, 167.67s/it]
  1%|          | 80/7180 [3:57:02<311:58:27, 158.18s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.23991300015767328, 'learning_rate': 1.99938742891813e-05, 'completion_length': 374.75, 'rewards/accuracy_reward': 0.3042483776807785, 'rewards/format_reward': 1.0, 'reward': 1.3042483925819397, 'reward_std': 0.46843069791793823, 'kl': 0.0147705078125, 'epoch': 0.22}

  1%|          | 80/7180 [3:57:02<311:58:27, 158.18s/it]
  1%|          | 81/7180 [3:59:01<288:34:11, 146.34s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.2287270837235596, 'learning_rate': 1.9993720205399113e-05, 'completion_length': 358.97918701171875, 'rewards/accuracy_reward': 0.4836186468601227, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4627854228019714, 'reward_std': 0.28989897295832634, 'kl': 0.014862060546875, 'epoch': 0.23}

  1%|          | 81/7180 [3:59:01<288:34:11, 146.34s/it]
  1%|          | 82/7180 [4:01:02<273:36:12, 138.77s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.2386615757606059, 'learning_rate': 1.999356420833911e-05, 'completion_length': 376.7291717529297, 'rewards/accuracy_reward': 0.4402586668729782, 'rewards/format_reward': 1.0, 'reward': 1.4402587413787842, 'reward_std': 0.4158908277750015, 'kl': 0.014862060546875, 'epoch': 0.23}

  1%|          | 82/7180 [4:01:02<273:36:12, 138.77s/it]
  1%|          | 83/7180 [4:03:00<261:20:40, 132.57s/it]
                                                        
{'loss': 0.0006, 'grad_norm': 0.23489840583332858, 'learning_rate': 1.999340629803116e-05, 'completion_length': 330.93751525878906, 'rewards/accuracy_reward': 0.4992893934249878, 'rewards/format_reward': 1.0, 'reward': 1.4992894530296326, 'reward_std': 0.4557451009750366, 'kl': 0.016021728515625, 'epoch': 0.23}

  1%|          | 83/7180 [4:03:00<261:20:40, 132.57s/it]
  1%|          | 84/7180 [4:05:05<256:53:07, 130.33s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.20937427505280565, 'learning_rate': 1.9993246474505485e-05, 'completion_length': 348.0208435058594, 'rewards/accuracy_reward': 0.43627454340457916, 'rewards/format_reward': 1.0, 'reward': 1.4362745881080627, 'reward_std': 0.33632509410381317, 'kl': 0.01715087890625, 'epoch': 0.23}

  1%|          | 84/7180 [4:05:05<256:53:07, 130.33s/it]
  1%|          | 85/7180 [4:07:26<263:19:48, 133.61s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.21492250302188204, 'learning_rate': 1.9993084737792687e-05, 'completion_length': 357.6875, 'rewards/accuracy_reward': 0.4223310649394989, 'rewards/format_reward': 1.0, 'reward': 1.4223310947418213, 'reward_std': 0.28418760001659393, 'kl': 0.01629638671875, 'epoch': 0.24}

  1%|          | 85/7180 [4:07:26<263:19:48, 133.61s/it]
  1%|          | 86/7180 [4:09:32<258:56:11, 131.40s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.19655368444800814, 'learning_rate': 1.999292108792373e-05, 'completion_length': 359.68751525878906, 'rewards/accuracy_reward': 0.3207145258784294, 'rewards/format_reward': 1.0, 'reward': 1.32071453332901, 'reward_std': 0.32943516224622726, 'kl': 0.0169677734375, 'epoch': 0.24}

  1%|          | 86/7180 [4:09:32<258:56:11, 131.40s/it]
  1%|          | 87/7180 [4:11:33<252:40:26, 128.24s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.18067659008341502, 'learning_rate': 1.9992755524929946e-05, 'completion_length': 356.66668701171875, 'rewards/accuracy_reward': 0.23294468969106674, 'rewards/format_reward': 1.0, 'reward': 1.232944667339325, 'reward_std': 0.2819448933005333, 'kl': 0.016357421875, 'epoch': 0.24}

  1%|          | 87/7180 [4:11:33<252:40:26, 128.24s/it]
  1%|          | 88/7180 [4:13:40<251:50:47, 127.84s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.23762012590405993, 'learning_rate': 1.9992588048843033e-05, 'completion_length': 357.5416717529297, 'rewards/accuracy_reward': 0.5456196963787079, 'rewards/format_reward': 1.0, 'reward': 1.545619785785675, 'reward_std': 0.40676823258399963, 'kl': 0.0198974609375, 'epoch': 0.24}

  1%|          | 88/7180 [4:13:40<251:50:47, 127.84s/it]
  1%|          | 89/7180 [4:15:50<253:14:08, 128.56s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.2360782813037787, 'learning_rate': 1.999241865969505e-05, 'completion_length': 364.62501525878906, 'rewards/accuracy_reward': 0.499671995639801, 'rewards/format_reward': 1.0, 'reward': 1.4996719360351562, 'reward_std': 0.35570600628852844, 'kl': 0.0164794921875, 'epoch': 0.25}

  1%|          | 89/7180 [4:15:50<253:14:08, 128.56s/it]
  1%|▏         | 90/7180 [4:18:13<261:22:36, 132.72s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.25275441682195254, 'learning_rate': 1.9992247357518428e-05, 'completion_length': 343.3125, 'rewards/accuracy_reward': 0.537217989563942, 'rewards/format_reward': 1.0, 'reward': 1.5372180342674255, 'reward_std': 0.3841910809278488, 'kl': 0.01806640625, 'epoch': 0.25}

  1%|▏         | 90/7180 [4:18:13<261:22:36, 132.72s/it]
  1%|▏         | 91/7180 [4:20:12<253:28:42, 128.72s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.22788611673525122, 'learning_rate': 1.9992074142345964e-05, 'completion_length': 346.91668701171875, 'rewards/accuracy_reward': 0.6028910577297211, 'rewards/format_reward': 1.0, 'reward': 1.6028911471366882, 'reward_std': 0.36899957060813904, 'kl': 0.02032470703125, 'epoch': 0.25}

  1%|▏         | 91/7180 [4:20:12<253:28:42, 128.72s/it]
  1%|▏         | 92/7180 [4:22:05<243:53:47, 123.88s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.18254207112445364, 'learning_rate': 1.9991899014210815e-05, 'completion_length': 339.22918701171875, 'rewards/accuracy_reward': 0.4669249653816223, 'rewards/format_reward': 1.0, 'reward': 1.466925024986267, 'reward_std': 0.19788773357868195, 'kl': 0.0194091796875, 'epoch': 0.26}

  1%|▏         | 92/7180 [4:22:05<243:53:47, 123.88s/it]
  1%|▏         | 93/7180 [4:24:14<246:43:31, 125.33s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.14849319514714301, 'learning_rate': 1.9991721973146515e-05, 'completion_length': 361.4791717529297, 'rewards/accuracy_reward': 0.3645287752151489, 'rewards/format_reward': 1.0, 'reward': 1.3645288944244385, 'reward_std': 0.09900659695267677, 'kl': 0.02081298828125, 'epoch': 0.26}

  1%|▏         | 93/7180 [4:24:14<246:43:31, 125.33s/it]
  1%|▏         | 94/7180 [4:26:22<248:26:48, 126.22s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.21547131988150153, 'learning_rate': 1.9991543019186954e-05, 'completion_length': 366.1666717529297, 'rewards/accuracy_reward': 0.453312486410141, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.4116458296775818, 'reward_std': 0.38181858509778976, 'kl': 0.01910400390625, 'epoch': 0.26}

  1%|▏         | 94/7180 [4:26:22<248:26:48, 126.22s/it]
  1%|▏         | 95/7180 [4:28:22<245:01:47, 124.50s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.17570245261951642, 'learning_rate': 1.9991362152366393e-05, 'completion_length': 353.2708435058594, 'rewards/accuracy_reward': 0.3950163722038269, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.35334974527359, 'reward_std': 0.21407321095466614, 'kl': 0.0203857421875, 'epoch': 0.26}

  1%|▏         | 95/7180 [4:28:22<245:01:47, 124.50s/it]
  1%|▏         | 96/7180 [4:30:26<244:26:30, 124.22s/it]
                                                        
{'loss': 0.0007, 'grad_norm': 0.17733213160459646, 'learning_rate': 1.999117937271946e-05, 'completion_length': 364.4375, 'rewards/accuracy_reward': 0.5008310973644257, 'rewards/format_reward': 1.0, 'reward': 1.5008311867713928, 'reward_std': 0.21475445479154587, 'kl': 0.0185546875, 'epoch': 0.27}

  1%|▏         | 96/7180 [4:30:26<244:26:30, 124.22s/it]
  1%|▏         | 97/7180 [4:32:34<246:49:08, 125.45s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.1854306614836212, 'learning_rate': 1.9990994680281148e-05, 'completion_length': 354.8333435058594, 'rewards/accuracy_reward': 0.6034824103116989, 'rewards/format_reward': 1.0, 'reward': 1.60348242521286, 'reward_std': 0.25733324885368347, 'kl': 0.01947021484375, 'epoch': 0.27}

  1%|▏         | 97/7180 [4:32:34<246:49:08, 125.45s/it]
  1%|▏         | 98/7180 [4:34:31<241:39:25, 122.84s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2082362840448404, 'learning_rate': 1.9990808075086815e-05, 'completion_length': 337.04168701171875, 'rewards/accuracy_reward': 0.37377454340457916, 'rewards/format_reward': 1.0, 'reward': 1.3737745881080627, 'reward_std': 0.3144879788160324, 'kl': 0.020263671875, 'epoch': 0.27}

  1%|▏         | 98/7180 [4:34:31<241:39:25, 122.84s/it]
  1%|▏         | 99/7180 [4:36:26<236:44:37, 120.36s/it]
                                                        
{'loss': 0.0008, 'grad_norm': 0.2022471082509493, 'learning_rate': 1.9990619557172188e-05, 'completion_length': 345.0833435058594, 'rewards/accuracy_reward': 0.32864847779273987, 'rewards/format_reward': 1.0, 'reward': 1.3286485075950623, 'reward_std': 0.33764636516571045, 'kl': 0.02117919921875, 'epoch': 0.28}

  1%|▏         | 99/7180 [4:36:26<236:44:37, 120.36s/it]
  1%|▏         | 100/7180 [4:38:22<234:09:53, 119.07s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.19371493272949922, 'learning_rate': 1.9990429126573353e-05, 'completion_length': 349.5208435058594, 'rewards/accuracy_reward': 0.4982741177082062, 'rewards/format_reward': 1.0, 'reward': 1.4982741475105286, 'reward_std': 0.23949052393436432, 'kl': 0.0172119140625, 'epoch': 0.28}

  1%|▏         | 100/7180 [4:38:22<234:09:53, 119.07s/it]/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")
/data3//miniconda3/envs/Vlm-r1/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

  1%|▏         | 101/7180 [4:40:49<250:56:42, 127.62s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.226986426840633, 'learning_rate': 1.999023678332677e-05, 'completion_length': 357.31251525878906, 'rewards/accuracy_reward': 0.42734597623348236, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4065126776695251, 'reward_std': 0.4895176440477371, 'kl': 0.0233154296875, 'epoch': 0.28}

  1%|▏         | 101/7180 [4:40:49<250:56:42, 127.62s/it]
  1%|▏         | 102/7180 [4:42:43<242:59:20, 123.59s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.2473540500722477, 'learning_rate': 1.9990042527469268e-05, 'completion_length': 338.18751525878906, 'rewards/accuracy_reward': 0.35581474006175995, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3349815011024475, 'reward_std': 0.4118894785642624, 'kl': 0.0196533203125, 'epoch': 0.28}

  1%|▏         | 102/7180 [4:42:43<242:59:20, 123.59s/it]
  1%|▏         | 103/7180 [4:44:47<243:14:20, 123.73s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.21712972758954754, 'learning_rate': 1.998984635903803e-05, 'completion_length': 372.3333435058594, 'rewards/accuracy_reward': 0.44112076610326767, 'rewards/format_reward': 1.0, 'reward': 1.4411208629608154, 'reward_std': 0.34179918467998505, 'kl': 0.01751708984375, 'epoch': 0.29}

  1%|▏         | 103/7180 [4:44:47<243:14:20, 123.73s/it]
  1%|▏         | 104/7180 [4:46:54<244:57:19, 124.62s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.21300836019745328, 'learning_rate': 1.9989648278070617e-05, 'completion_length': 345.56251525878906, 'rewards/accuracy_reward': 0.43833106756210327, 'rewards/format_reward': 1.0, 'reward': 1.4383311867713928, 'reward_std': 0.39542557299137115, 'kl': 0.02099609375, 'epoch': 0.29}

  1%|▏         | 104/7180 [4:46:54<244:57:19, 124.62s/it]
  1%|▏         | 105/7180 [4:48:44<236:04:33, 120.12s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.24643563675207647, 'learning_rate': 1.9989448284604947e-05, 'completion_length': 350.3541717529297, 'rewards/accuracy_reward': 0.5888638198375702, 'rewards/format_reward': 1.0, 'reward': 1.5888637900352478, 'reward_std': 0.42679864168167114, 'kl': 0.02099609375, 'epoch': 0.29}

  1%|▏         | 105/7180 [4:48:44<236:04:33, 120.12s/it]
  1%|▏         | 106/7180 [4:50:36<231:28:46, 117.80s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.17177349136971357, 'learning_rate': 1.9989246378679314e-05, 'completion_length': 327.1666717529297, 'rewards/accuracy_reward': 0.4467662386596203, 'rewards/format_reward': 1.0, 'reward': 1.4467662572860718, 'reward_std': 0.22829151153564453, 'kl': 0.019775390625, 'epoch': 0.29}

  1%|▏         | 106/7180 [4:50:36<231:28:46, 117.80s/it]
  1%|▏         | 107/7180 [4:52:42<236:02:26, 120.14s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.20594474450829758, 'learning_rate': 1.9989042560332365e-05, 'completion_length': 362.31251525878906, 'rewards/accuracy_reward': 0.56563501060009, 'rewards/format_reward': 1.0, 'reward': 1.565635085105896, 'reward_std': 0.3792641758918762, 'kl': 0.0208740234375, 'epoch': 0.3}

  1%|▏         | 107/7180 [4:52:42<236:02:26, 120.14s/it]
  2%|▏         | 108/7180 [4:54:45<237:44:41, 121.02s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.2059522527341495, 'learning_rate': 1.9988836829603126e-05, 'completion_length': 340.75001525878906, 'rewards/accuracy_reward': 0.229166679084301, 'rewards/format_reward': 1.0, 'reward': 1.2291667461395264, 'reward_std': 0.35457347333431244, 'kl': 0.01873779296875, 'epoch': 0.3}

  2%|▏         | 108/7180 [4:54:45<237:44:41, 121.02s/it]
  2%|▏         | 109/7180 [4:56:51<240:47:57, 122.60s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.20032014778216095, 'learning_rate': 1.998862918653098e-05, 'completion_length': 351.2291717529297, 'rewards/accuracy_reward': 0.5833333432674408, 'rewards/format_reward': 1.0, 'reward': 1.5833333730697632, 'reward_std': 0.2957112416625023, 'kl': 0.0194091796875, 'epoch': 0.3}

  2%|▏         | 109/7180 [4:56:51<240:47:57, 122.60s/it]
  2%|▏         | 110/7180 [4:59:06<248:04:12, 126.32s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.1590269269381602, 'learning_rate': 1.9988419631155686e-05, 'completion_length': 372.6666717529297, 'rewards/accuracy_reward': 0.3325163573026657, 'rewards/format_reward': 1.0, 'reward': 1.332516372203827, 'reward_std': 0.23066146671772003, 'kl': 0.02191162109375, 'epoch': 0.31}

  2%|▏         | 110/7180 [4:59:06<248:04:12, 126.32s/it]
  2%|▏         | 111/7180 [5:01:08<245:35:41, 125.07s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.2083031283791386, 'learning_rate': 1.9988208163517357e-05, 'completion_length': 352.6666717529297, 'rewards/accuracy_reward': 0.3541666716337204, 'rewards/format_reward': 1.0, 'reward': 1.3541666865348816, 'reward_std': 0.35457347333431244, 'kl': 0.01904296875, 'epoch': 0.31}

  2%|▏         | 111/7180 [5:01:08<245:35:41, 125.07s/it]
  2%|▏         | 112/7180 [5:03:06<241:25:13, 122.96s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.20089983422775406, 'learning_rate': 1.998799478365648e-05, 'completion_length': 332.75001525878906, 'rewards/accuracy_reward': 0.48256176710128784, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4617284536361694, 'reward_std': 0.3468383252620697, 'kl': 0.02032470703125, 'epoch': 0.31}

  2%|▏         | 112/7180 [5:03:06<241:25:13, 122.96s/it]
  2%|▏         | 113/7180 [5:05:17<246:03:51, 125.35s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.15825303131158722, 'learning_rate': 1.998777949161391e-05, 'completion_length': 346.2083435058594, 'rewards/accuracy_reward': 0.8325163722038269, 'rewards/format_reward': 1.0, 'reward': 1.832516372203827, 'reward_std': 0.18441854417324066, 'kl': 0.0208740234375, 'epoch': 0.31}

  2%|▏         | 113/7180 [5:05:17<246:03:51, 125.35s/it]
  2%|▏         | 114/7180 [5:07:27<248:50:01, 126.78s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.21918666793755412, 'learning_rate': 1.9987562287430853e-05, 'completion_length': 355.43751525878906, 'rewards/accuracy_reward': 0.6450163722038269, 'rewards/format_reward': 1.0, 'reward': 1.645016372203827, 'reward_std': 0.435925230383873, 'kl': 0.0218505859375, 'epoch': 0.32}

  2%|▏         | 114/7180 [5:07:27<248:50:01, 126.78s/it]
  2%|▏         | 115/7180 [5:09:30<246:38:59, 125.68s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.2176066952608199, 'learning_rate': 1.9987343171148904e-05, 'completion_length': 342.1458435058594, 'rewards/accuracy_reward': 0.4583333432674408, 'rewards/format_reward': 1.0, 'reward': 1.4583333730697632, 'reward_std': 0.3776952475309372, 'kl': 0.01953125, 'epoch': 0.32}

  2%|▏         | 115/7180 [5:09:30<246:38:59, 125.68s/it]
  2%|▏         | 116/7180 [5:11:26<240:35:03, 122.61s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.22540414422919158, 'learning_rate': 1.998712214281001e-05, 'completion_length': 331.3958435058594, 'rewards/accuracy_reward': 0.6341911852359772, 'rewards/format_reward': 1.0, 'reward': 1.6341912150382996, 'reward_std': 0.40390540659427643, 'kl': 0.01959228515625, 'epoch': 0.32}

  2%|▏         | 116/7180 [5:11:26<240:35:03, 122.61s/it]
  2%|▏         | 117/7180 [5:13:40<247:27:04, 126.13s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.2093341947309171, 'learning_rate': 1.998689920245648e-05, 'completion_length': 362.4791717529297, 'rewards/accuracy_reward': 0.47875818610191345, 'rewards/format_reward': 1.0, 'reward': 1.4787582159042358, 'reward_std': 0.4362480938434601, 'kl': 0.0213623046875, 'epoch': 0.33}

  2%|▏         | 117/7180 [5:13:40<247:27:04, 126.13s/it]
  2%|▏         | 118/7180 [5:15:38<242:36:14, 123.67s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.2042583350691608, 'learning_rate': 1.9986674350131005e-05, 'completion_length': 352.00001525878906, 'rewards/accuracy_reward': 0.4417211711406708, 'rewards/format_reward': 1.0, 'reward': 1.4417212009429932, 'reward_std': 0.28017377108335495, 'kl': 0.02142333984375, 'epoch': 0.33}

  2%|▏         | 118/7180 [5:15:38<242:36:14, 123.67s/it]
  2%|▏         | 119/7180 [5:17:39<240:39:45, 122.70s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.24298780667345116, 'learning_rate': 1.9986447585876627e-05, 'completion_length': 318.7708435058594, 'rewards/accuracy_reward': 0.4898109883069992, 'rewards/format_reward': 1.0, 'reward': 1.4898109436035156, 'reward_std': 0.34272758662700653, 'kl': 0.02313232421875, 'epoch': 0.33}

  2%|▏         | 119/7180 [5:17:39<240:39:45, 122.70s/it]
  2%|▏         | 120/7180 [5:19:41<240:17:01, 122.52s/it]
                                                         
{'loss': 0.001, 'grad_norm': 0.2114967851506301, 'learning_rate': 1.9986218909736758e-05, 'completion_length': 334.9375, 'rewards/accuracy_reward': 0.6325757801532745, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.611742377281189, 'reward_std': 0.40868210792541504, 'kl': 0.02471923828125, 'epoch': 0.33}

  2%|▏         | 120/7180 [5:19:41<240:17:01, 122.52s/it]
  2%|▏         | 121/7180 [5:21:42<239:48:59, 122.30s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.2125344509316606, 'learning_rate': 1.998598832175518e-05, 'completion_length': 326.18751525878906, 'rewards/accuracy_reward': 0.39542485773563385, 'rewards/format_reward': 1.0, 'reward': 1.3954249024391174, 'reward_std': 0.37181707471609116, 'kl': 0.02203369140625, 'epoch': 0.34}

  2%|▏         | 121/7180 [5:21:42<239:48:59, 122.30s/it]
  2%|▏         | 122/7180 [5:23:52<244:00:25, 124.46s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.19304806603293473, 'learning_rate': 1.9985755821976037e-05, 'completion_length': 359.25001525878906, 'rewards/accuracy_reward': 0.4571540802717209, 'rewards/format_reward': 1.0, 'reward': 1.4571541547775269, 'reward_std': 0.359762579202652, 'kl': 0.02008056640625, 'epoch': 0.34}

  2%|▏         | 122/7180 [5:23:52<244:00:25, 124.46s/it]
  2%|▏         | 123/7180 [5:26:03<247:37:38, 126.32s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.20428177730256605, 'learning_rate': 1.9985521410443846e-05, 'completion_length': 348.0208435058594, 'rewards/accuracy_reward': 0.2708333432674408, 'rewards/format_reward': 1.0, 'reward': 1.2708333730697632, 'reward_std': 0.36417655646800995, 'kl': 0.01947021484375, 'epoch': 0.34}

  2%|▏         | 123/7180 [5:26:03<247:37:38, 126.32s/it]
  2%|▏         | 124/7180 [5:28:06<245:35:56, 125.31s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.18316669135706523, 'learning_rate': 1.998528508720348e-05, 'completion_length': 352.7708435058594, 'rewards/accuracy_reward': 0.7071079015731812, 'rewards/format_reward': 1.0, 'reward': 1.7071079015731812, 'reward_std': 0.25915516912937164, 'kl': 0.0205078125, 'epoch': 0.34}

  2%|▏         | 124/7180 [5:28:06<245:35:56, 125.31s/it]
  2%|▏         | 125/7180 [5:30:17<249:04:36, 127.10s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.240523557694088, 'learning_rate': 1.9985046852300183e-05, 'completion_length': 340.1666717529297, 'rewards/accuracy_reward': 0.6233660578727722, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.6025327444076538, 'reward_std': 0.42000287771224976, 'kl': 0.02166748046875, 'epoch': 0.35}

  2%|▏         | 125/7180 [5:30:17<249:04:36, 127.10s/it]
  2%|▏         | 126/7180 [5:32:29<252:14:55, 128.73s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.20273724841587654, 'learning_rate': 1.9984806705779563e-05, 'completion_length': 376.2708435058594, 'rewards/accuracy_reward': 0.49836602807044983, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4775327444076538, 'reward_std': 0.39454828202724457, 'kl': 0.023681640625, 'epoch': 0.35}

  2%|▏         | 126/7180 [5:32:29<252:14:55, 128.73s/it]
  2%|▏         | 127/7180 [5:34:37<251:18:45, 128.28s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.22353489881545913, 'learning_rate': 1.9984564647687595e-05, 'completion_length': 356.7291717529297, 'rewards/accuracy_reward': 0.3998108208179474, 'rewards/format_reward': 1.0, 'reward': 1.3998108506202698, 'reward_std': 0.41083547472953796, 'kl': 0.01995849609375, 'epoch': 0.35}

  2%|▏         | 127/7180 [5:34:37<251:18:45, 128.28s/it]
  2%|▏         | 128/7180 [5:36:43<250:00:59, 127.63s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.17238430255194165, 'learning_rate': 1.998432067807063e-05, 'completion_length': 378.5416717529297, 'rewards/accuracy_reward': 0.4575163573026657, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4366830587387085, 'reward_std': 0.34611886739730835, 'kl': 0.0224609375, 'epoch': 0.36}

  2%|▏         | 128/7180 [5:36:43<250:00:59, 127.63s/it]
  2%|▏         | 129/7180 [5:38:58<254:19:18, 129.85s/it]
                                                         
{'loss': 0.001, 'grad_norm': 0.23349539676268943, 'learning_rate': 1.9984074796975363e-05, 'completion_length': 402.97918701171875, 'rewards/accuracy_reward': 0.5494666695594788, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.5286334156990051, 'reward_std': 0.4549315869808197, 'kl': 0.0245361328125, 'epoch': 0.36}

  2%|▏         | 129/7180 [5:38:58<254:19:18, 129.85s/it]
  2%|▏         | 130/7180 [5:40:59<248:58:38, 127.14s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.21277238540342958, 'learning_rate': 1.9983827004448875e-05, 'completion_length': 353.9791717529297, 'rewards/accuracy_reward': 0.520833358168602, 'rewards/format_reward': 1.0, 'reward': 1.5208333730697632, 'reward_std': 0.39121395349502563, 'kl': 0.0181884765625, 'epoch': 0.36}

  2%|▏         | 130/7180 [5:40:59<248:58:38, 127.14s/it]
  2%|▏         | 131/7180 [5:43:11<252:16:41, 128.84s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.20972741110716372, 'learning_rate': 1.9983577300538603e-05, 'completion_length': 372.7083435058594, 'rewards/accuracy_reward': 0.6073296368122101, 'rewards/format_reward': 1.0, 'reward': 1.6073296666145325, 'reward_std': 0.41966868937015533, 'kl': 0.019775390625, 'epoch': 0.36}

  2%|▏         | 131/7180 [5:43:11<252:16:41, 128.84s/it]
  2%|▏         | 132/7180 [5:45:18<250:50:16, 128.12s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.19176626797887011, 'learning_rate': 1.9983325685292356e-05, 'completion_length': 418.56251525878906, 'rewards/accuracy_reward': 0.520424872636795, 'rewards/format_reward': 1.0, 'reward': 1.5204249024391174, 'reward_std': 0.4111013412475586, 'kl': 0.018798828125, 'epoch': 0.37}

  2%|▏         | 132/7180 [5:45:18<250:50:16, 128.12s/it]
  2%|▏         | 133/7180 [5:47:27<251:41:26, 128.58s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.1638185906789527, 'learning_rate': 1.9983072158758293e-05, 'completion_length': 387.00001525878906, 'rewards/accuracy_reward': 0.42480939626693726, 'rewards/format_reward': 1.0, 'reward': 1.4248093962669373, 'reward_std': 0.21899615228176117, 'kl': 0.01824951171875, 'epoch': 0.37}

  2%|▏         | 133/7180 [5:47:27<251:41:26, 128.58s/it]
  2%|▏         | 134/7180 [5:49:31<248:25:59, 126.93s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.21539976526096083, 'learning_rate': 1.9982816720984966e-05, 'completion_length': 376.7291717529297, 'rewards/accuracy_reward': 0.5090130567550659, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.4881796836853027, 'reward_std': 0.4703689217567444, 'kl': 0.01873779296875, 'epoch': 0.37}

  2%|▏         | 134/7180 [5:49:31<248:25:59, 126.93s/it]
  2%|▏         | 135/7180 [5:51:42<250:49:21, 128.17s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.19785578973807375, 'learning_rate': 1.9982559372021274e-05, 'completion_length': 393.54168701171875, 'rewards/accuracy_reward': 0.5226625800132751, 'rewards/format_reward': 1.0, 'reward': 1.5226625800132751, 'reward_std': 0.43207886815071106, 'kl': 0.01788330078125, 'epoch': 0.38}

  2%|▏         | 135/7180 [5:51:42<250:49:21, 128.17s/it]
  2%|▏         | 136/7180 [5:53:48<249:44:10, 127.63s/it]
                                                         
{'loss': 0.0007, 'grad_norm': 0.18989428032963349, 'learning_rate': 1.998230011191648e-05, 'completion_length': 388.06251525878906, 'rewards/accuracy_reward': 0.6654412150382996, 'rewards/format_reward': 1.0, 'reward': 1.6654412150382996, 'reward_std': 0.3040429800748825, 'kl': 0.01849365234375, 'epoch': 0.38}

  2%|▏         | 136/7180 [5:53:48<249:44:10, 127.63s/it]
  2%|▏         | 137/7180 [5:56:01<253:01:25, 129.33s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.2140002988249503, 'learning_rate': 1.9982038940720226e-05, 'completion_length': 390.62501525878906, 'rewards/accuracy_reward': 0.44002988934516907, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.3983632922172546, 'reward_std': 0.4851021319627762, 'kl': 0.02264404296875, 'epoch': 0.38}

  2%|▏         | 137/7180 [5:56:01<253:01:25, 129.33s/it]
  2%|▏         | 138/7180 [5:58:14<255:09:17, 130.44s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.20853827146801338, 'learning_rate': 1.9981775858482506e-05, 'completion_length': 387.6041717529297, 'rewards/accuracy_reward': 0.49959151446819305, 'rewards/format_reward': 1.0, 'reward': 1.4995915293693542, 'reward_std': 0.3612615764141083, 'kl': 0.02166748046875, 'epoch': 0.38}

  2%|▏         | 138/7180 [5:58:14<255:09:17, 130.44s/it]
  2%|▏         | 139/7180 [6:00:28<257:08:41, 131.48s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.19642759349535216, 'learning_rate': 1.9981510865253692e-05, 'completion_length': 414.5208435058594, 'rewards/accuracy_reward': 0.45792484283447266, 'rewards/format_reward': 0.9583333730697632, 'reward': 1.4162582159042358, 'reward_std': 0.3859797418117523, 'kl': 0.0224609375, 'epoch': 0.39}

  2%|▏         | 139/7180 [6:00:28<257:08:41, 131.48s/it]
  2%|▏         | 140/7180 [6:02:35<254:06:53, 129.95s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.20996880648366456, 'learning_rate': 1.9981243961084516e-05, 'completion_length': 391.91668701171875, 'rewards/accuracy_reward': 0.3125, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.2916666865348816, 'reward_std': 0.3922024667263031, 'kl': 0.02313232421875, 'epoch': 0.39}

  2%|▏         | 140/7180 [6:02:35<254:06:53, 129.95s/it]
  2%|▏         | 141/7180 [6:04:43<252:59:07, 129.39s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.21194154635790344, 'learning_rate': 1.9980975146026073e-05, 'completion_length': 365.5, 'rewards/accuracy_reward': 0.7569046020507812, 'rewards/format_reward': 1.0, 'reward': 1.7569046020507812, 'reward_std': 0.2816191241145134, 'kl': 0.0213623046875, 'epoch': 0.39}

  2%|▏         | 141/7180 [6:04:43<252:59:07, 129.39s/it]
  2%|▏         | 142/7180 [6:07:12<264:28:16, 135.28s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.19949929605821684, 'learning_rate': 1.998070442012983e-05, 'completion_length': 437.3541717529297, 'rewards/accuracy_reward': 0.533681109547615, 'rewards/format_reward': 1.0, 'reward': 1.5336812138557434, 'reward_std': 0.4882856011390686, 'kl': 0.02056884765625, 'epoch': 0.39}

  2%|▏         | 142/7180 [6:07:12<264:28:16, 135.28s/it]
  2%|▏         | 143/7180 [6:09:33<267:52:23, 137.04s/it]
                                                         
{'loss': 0.0012, 'grad_norm': 0.2372734880222283, 'learning_rate': 1.9980431783447615e-05, 'completion_length': 397.2083435058594, 'rewards/accuracy_reward': 0.6257733702659607, 'rewards/format_reward': 0.9375000298023224, 'reward': 1.5632734298706055, 'reward_std': 0.5114598423242569, 'kl': 0.0296630859375, 'epoch': 0.4}

  2%|▏         | 143/7180 [6:09:33<267:52:23, 137.04s/it]
  2%|▏         | 144/7180 [6:11:54<270:31:44, 138.42s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.17626211892892904, 'learning_rate': 1.9980157236031624e-05, 'completion_length': 390.31251525878906, 'rewards/accuracy_reward': 0.37459151446819305, 'rewards/format_reward': 0.9583333432674408, 'reward': 1.3329248428344727, 'reward_std': 0.38786081969738007, 'kl': 0.0213623046875, 'epoch': 0.4}

  2%|▏         | 144/7180 [6:11:54<270:31:44, 138.42s/it]
  2%|▏         | 145/7180 [6:14:23<276:37:46, 141.56s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.16574342574633763, 'learning_rate': 1.997988077793442e-05, 'completion_length': 434.3333435058594, 'rewards/accuracy_reward': 0.39542485773563385, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3745915293693542, 'reward_std': 0.23999332636594772, 'kl': 0.01898193359375, 'epoch': 0.4}

  2%|▏         | 145/7180 [6:14:23<276:37:46, 141.56s/it]
  2%|▏         | 146/7180 [6:16:56<283:21:47, 145.03s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.1892746163236171, 'learning_rate': 1.9979602409208928e-05, 'completion_length': 414.8333435058594, 'rewards/accuracy_reward': 0.6306585967540741, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.609825313091278, 'reward_std': 0.384250670671463, 'kl': 0.0218505859375, 'epoch': 0.41}

  2%|▏         | 146/7180 [6:16:56<283:21:47, 145.03s/it]
  2%|▏         | 147/7180 [6:19:12<277:35:45, 142.09s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.16775265616877175, 'learning_rate': 1.9979322129908444e-05, 'completion_length': 423.0833435058594, 'rewards/accuracy_reward': 0.23878904432058334, 'rewards/format_reward': 1.0, 'reward': 1.2387890815734863, 'reward_std': 0.2961459830403328, 'kl': 0.0218505859375, 'epoch': 0.41}

  2%|▏         | 147/7180 [6:19:12<277:35:45, 142.09s/it]
  2%|▏         | 148/7180 [6:21:26<272:46:49, 139.65s/it]
                                                         
{'loss': 0.001, 'grad_norm': 0.19448112612235272, 'learning_rate': 1.9979039940086628e-05, 'completion_length': 423.25, 'rewards/accuracy_reward': 0.6245915293693542, 'rewards/format_reward': 1.0, 'reward': 1.624591588973999, 'reward_std': 0.30454304814338684, 'kl': 0.02374267578125, 'epoch': 0.41}

  2%|▏         | 148/7180 [6:21:26<272:46:49, 139.65s/it]
  2%|▏         | 149/7180 [6:23:56<278:53:13, 142.80s/it]
                                                         
{'loss': 0.0009, 'grad_norm': 0.20608988633017053, 'learning_rate': 1.9978755839797496e-05, 'completion_length': 416.06251525878906, 'rewards/accuracy_reward': 0.5714486539363861, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.55061537027359, 'reward_std': 0.44047215580940247, 'kl': 0.02178955078125, 'epoch': 0.41}

  2%|▏         | 149/7180 [6:23:56<278:53:13, 142.80s/it]
  2%|▏         | 150/7180 [6:26:13<275:46:27, 141.22s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.1967754105307229, 'learning_rate': 1.997846982909545e-05, 'completion_length': 425.7083435058594, 'rewards/accuracy_reward': 0.3950163424015045, 'rewards/format_reward': 1.0, 'reward': 1.395016372203827, 'reward_std': 0.37315621972084045, 'kl': 0.01910400390625, 'epoch': 0.42}

  2%|▏         | 150/7180 [6:26:13<275:46:27, 141.22s/it]
  2%|▏         | 151/7180 [6:28:38<277:29:25, 142.12s/it]
                                                         
{'loss': 0.0008, 'grad_norm': 0.18350293059795308, 'learning_rate': 1.997818190803524e-05, 'completion_length': 423.25, 'rewards/accuracy_reward': 0.37937411665916443, 'rewards/format_reward': 0.9791666865348816, 'reward': 1.3585408329963684, 'reward_std': 0.39520296454429626, 'kl': 0.02044677734375, 'epoch': 0.42}

  2%|▏         | 151/7180 [6:28:38<277:29:25, 142.12s/it]Killed
